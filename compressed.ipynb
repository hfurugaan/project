{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 124\n",
      "Number of original features: 122\n",
      "Number of added zero features: 2\n",
      "Number of protocol_type categories: 3\n",
      "Number of service categories: 70\n",
      "Number of flag categories: 11\n",
      "All feature names:\n",
      "['count', 'diff_srv_rate', 'dst_bytes', 'dst_host_count', 'dst_host_diff_srv_rate', 'dst_host_rerror_rate', 'dst_host_same_src_port_rate', 'dst_host_same_srv_rate', 'dst_host_serror_rate', 'dst_host_srv_count', 'dst_host_srv_diff_host_rate', 'dst_host_srv_rerror_rate', 'dst_host_srv_serror_rate', 'duration', 'flag_OTH', 'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0', 'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH', 'hot', 'is_guest_login', 'is_host_login', 'land', 'logged_in', 'num_access_files', 'num_compromised', 'num_failed_logins', 'num_file_creations', 'num_outbound_cmds', 'num_root', 'num_shells', 'protocol_type_icmp', 'protocol_type_tcp', 'protocol_type_udp', 'rerror_rate', 'root_shell', 'same_srv_rate', 'serror_rate', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier', 'service_csnet_ns', 'service_ctf', 'service_daytime', 'service_discard', 'service_domain', 'service_domain_u', 'service_echo', 'service_eco_i', 'service_ecr_i', 'service_efs', 'service_exec', 'service_finger', 'service_ftp', 'service_ftp_data', 'service_gopher', 'service_harvest', 'service_hostnames', 'service_http', 'service_http_2784', 'service_http_443', 'service_http_8001', 'service_imap4', 'service_iso_tsap', 'service_klogin', 'service_kshell', 'service_ldap', 'service_link', 'service_login', 'service_mtp', 'service_name', 'service_netbios_dgm', 'service_netbios_ns', 'service_netbios_ssn', 'service_netstat', 'service_nnsp', 'service_nntp', 'service_ntp_u', 'service_other', 'service_pm_dump', 'service_pop_2', 'service_pop_3', 'service_printer', 'service_private', 'service_red_i', 'service_remote_job', 'service_rje', 'service_shell', 'service_smtp', 'service_sql_net', 'service_ssh', 'service_sunrpc', 'service_supdup', 'service_systat', 'service_telnet', 'service_tftp_u', 'service_tim_i', 'service_time', 'service_urh_i', 'service_urp_i', 'service_uucp', 'service_uucp_path', 'service_vmnet', 'service_whois', 'src_bytes', 'srv_count', 'srv_diff_host_rate', 'srv_rerror_rate', 'srv_serror_rate', 'su_attempted', 'urgent', 'wrong_fragment', 'added_feature_0', 'added_feature_1']\n",
      "Number of features in X_train_train: 124\n",
      "Number of features in X_test: 124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def prepare_nsl_kdd_data(train_path, test_path, validation_split=0.25, random_state=42):\n",
    "    # Define column names\n",
    "    columns = ['duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent','hot',\n",
    "               'num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root','num_file_creations',\n",
    "               'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count','srv_count',\n",
    "               'serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate','diff_srv_rate','srv_diff_host_rate',\n",
    "               'dst_host_count','dst_host_srv_count','dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate',\n",
    "               'dst_host_srv_diff_host_rate','dst_host_serror_rate','dst_host_srv_serror_rate','dst_host_rerror_rate',\n",
    "               'dst_host_srv_rerror_rate','attack','level']\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(train_path, sep=\",\", names=columns)\n",
    "    test_df = pd.read_csv(test_path, sep=\",\", names=columns)\n",
    "    \n",
    "    # Classify attacks\n",
    "    train_df['attack_state'] = train_df.attack.map(lambda a: 0 if a == 'normal' else 1)\n",
    "    test_df['attack_state'] = test_df.attack.map(lambda a: 0 if a == 'normal' else 1)\n",
    "    \n",
    "    # One-hot encoding\n",
    "    categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "    train_df_encoded = pd.get_dummies(train_df, columns=categorical_columns, prefix=categorical_columns, prefix_sep=\"_\")\n",
    "    test_df_encoded = pd.get_dummies(test_df, columns=categorical_columns, prefix=categorical_columns, prefix_sep=\"_\")\n",
    "    \n",
    "    # Ensure both train and test have the same columns\n",
    "    all_columns = set(train_df_encoded.columns) | set(test_df_encoded.columns)\n",
    "    for col in all_columns:\n",
    "        if col not in train_df_encoded.columns:\n",
    "            train_df_encoded[col] = 0\n",
    "        if col not in test_df_encoded.columns:\n",
    "            test_df_encoded[col] = 0\n",
    "    \n",
    "    # Ensure columns are in the same order\n",
    "    train_df_encoded = train_df_encoded.reindex(sorted(train_df_encoded.columns), axis=1)\n",
    "    test_df_encoded = test_df_encoded.reindex(sorted(train_df_encoded.columns), axis=1)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    drop_columns = ['attack', 'level', 'attack_state']\n",
    "    X_train = train_df_encoded.drop(drop_columns, axis=1)\n",
    "    Y_train = train_df_encoded['attack_state']\n",
    "    X_test = test_df_encoded.drop(drop_columns, axis=1)\n",
    "    Y_test = test_df_encoded['attack_state']\n",
    "    \n",
    "    # Add zero columns for missing features\n",
    "    current_feature_count = X_train.shape[1]\n",
    "    missing_feature_count = 124 - current_feature_count\n",
    "    \n",
    "    if missing_feature_count > 0:\n",
    "        for i in range(missing_feature_count):\n",
    "            column_name = f'added_feature_{i}'\n",
    "            X_train[column_name] = 0\n",
    "            X_test[column_name] = 0\n",
    "    \n",
    "    # Detailed diagnostic information\n",
    "    print(f\"Total number of features: {X_train.shape[1]}\")\n",
    "    print(f\"Number of original features: {current_feature_count}\")\n",
    "    print(f\"Number of added zero features: {missing_feature_count}\")\n",
    "    print(f\"Number of protocol_type categories: {len([col for col in X_train.columns if col.startswith('protocol_type_')])}\")\n",
    "    print(f\"Number of service categories: {len([col for col in X_train.columns if col.startswith('service_')])}\")\n",
    "    print(f\"Number of flag categories: {len([col for col in X_train.columns if col.startswith('flag_')])}\")\n",
    "    \n",
    "    # List all feature names\n",
    "    print(\"All feature names:\")\n",
    "    print(X_train.columns.tolist())\n",
    "    \n",
    "    # Ensure we have 124 features\n",
    "    assert X_train.shape[1] == 124, f\"Expected 124 features, but got {X_train.shape[1]}\"\n",
    "    assert X_test.shape[1] == 124, f\"Expected 124 features, but got {X_test.shape[1]}\"\n",
    "    \n",
    "    # Split training data into train and validation sets\n",
    "    X_train_train, X_test_train, Y_train_train, Y_test_train = train_test_split(X_train, Y_train, \n",
    "                                                                                test_size=validation_split, \n",
    "                                                                                random_state=random_state)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = RobustScaler()\n",
    "    X_train_train_scaled = scaler.fit_transform(X_train_train)\n",
    "    X_test_train_scaled = scaler.transform(X_test_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return (X_train_train_scaled, Y_train_train, X_test_train_scaled, Y_test_train, X_test_scaled, Y_test)\n",
    "\n",
    "# Usage example:\n",
    "X_train_train, Y_train_train, X_test_train, Y_test_train, X_test, Y_test = prepare_nsl_kdd_data(\"nsl-kdd-data/KDDTrain+.txt\", \"nsl-kdd-data/KDDTest+.txt\")\n",
    "\n",
    "# Print the number of features to verify\n",
    "print(f\"Number of features in X_train_train: {X_train_train.shape[1]}\")\n",
    "print(f\"Number of features in X_test: {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.5351982980344838\n",
      "Testing Score: 0.5325776338350162\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def load_model_and_predict(model_path, X_train, Y_train, X_test, Y_test):\n",
    "    # Load the model\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = loaded_model.predict(X_train)\n",
    "    test_predictions = loaded_model.predict(X_test)\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = accuracy_score(Y_train, train_predictions)\n",
    "    test_score = accuracy_score(Y_test, test_predictions)\n",
    "    \n",
    "    return train_score, test_score\n",
    "\n",
    "# Usage\n",
    "model_path = \"Random_Forest.pkl\"  # Path to your saved model\n",
    "train_score, test_score = load_model_and_predict(model_path, X_train_train, Y_train_train, X_test_train, Y_test_train)\n",
    "\n",
    "print(f\"Training Score: {train_score}\")\n",
    "print(f\"Testing Score: {test_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
